---
- name: Ensure all nodes are in Ready state
  run_once: true
  block:
  - name: Get all node status
    raw: "kubectl get nodes --no-headers"
    register: all_nodes_status
  - name: Get all node status different than Ready
    raw: echo "{{ all_nodes_status.stdout }}" | grep -v ' Ready'
    register: nodes_status
  - name: Show not ready nodes
    debug: msg="{{ nodes_status.stdout_lines }}"
    when: nodes_status.stdout != "\r\n"
  - name: Confirm Upgrade
    pause:
      prompt: "Some workers are not in Ready state, do you want to continue with upgrade? Type 'yes' to continue"
    register: pause_result
    when: nodes_status.stdout != "\r\n"
  - name: Fail if user does not confirm upgrade
    fail:
      msg: "Upgrade will not continue"
    when: pause_result.user_input | default('yes') != 'yes'

- name: Check api is up
  uri:
    url: "https://{{ ip | default(fallback_ips[inventory_hostname]) }}:{{ kube_apiserver_port }}/healthz"
    validate_certs: false
  when: first_master_node
  register: _result
  retries: 60
  delay: 5
  until: _result.status == 200
  tags: ['reconfigure-cluster']

- name: Check if kubeadm has already run
  stat:
    path: "/var/lib/kubelet/config.yaml"
    get_attributes: false
    get_checksum: false
    get_mime: false
  register: kubeadm_already_run
  tags: ['reconfigure-cluster']

- name: "Backup Kubernetes configuration files"
  archive:
    path: "{{ kube_config_dir }}"
    dest: "{{ kube_config_dir }}/kubernetes-config-{{ ansible_date_time.date }}.tar.gz"
    format: gz
  tags: ['reconfigure-cluster']

- name: Create backup directory if it doesn't exist
  file:
    path: "{{ kube_reconfig_dir }}"
    state: directory
    mode: '0755'
    owner: root
    group: root
  tags: ['reconfigure-cluster']

- name: Backup ConfigMap
  when: first_master_node
  run_once: true
  ansible.builtin.shell: >
    kubectl get cm -n {{ cm_namespace }} {{ cm_name }} -o yaml > {{ kube_reconfig_path }}
  become: yes
  register: cm_backup
  failed_when: cm_backup.rc != 0

- name: Delete ConfigMap
  when: first_master_node and cm_backup.rc == 0
  run_once: true
  ansible.builtin.command: >
    kubectl delete cm -n {{ cm_namespace }} {{ cm_name }}
  become: yes
  register: cm_delete
  failed_when: cm_delete.rc != 0

- name: Kubeadm | Create reconfigure kubeadm config
  template:
    src: "cluster-configuration-replace.yaml.j2"
    dest: "{{ kube_reconfig_path }}/kubeadm-cluster-config-replace.yaml"
    mode: "0640"
  tags: ['reconfigure-cluster']

- name: Kubeadm | Create new configmap for ClusterConfiguration
  command: "kubectl create configmap -n {{ cm_namespace }} {{ cm_name }} --from-file=ClusterConfiguration={{ kube_reconfig_path }}/kubeadm-cluster-config-replace.yaml"
  run_once: true
  when: first_master_node
  tags: ['reconfigure-cluster']

- name: Reconfigure control-plane
  when:
    - kubeadm_already_run.stat.exists
  tags: ['reconfigure-cluster']
  block:
    - name: Kubeadm | Prepare ClusterConfiguration manifest
      lineinfile:
        path: "{{ kube_reconfig_path }}/kubeadm-cluster-config-replace.yaml"
        line: "{{ item }}"
        create: yes
        loop:
          - "apiVersion: kubeadm.k8s.io/v1beta3"
          - "kind: ClusterConfiguration"

    - name: Kubeadm | Write new manifest for kubernetes control plane components
      command: "kubeadm init phase control-plane all --config {{ kube_reconfig_path }}/kubeadm-cluster-config-replace.yaml"

    - name: Kubeadm | Reconfigure local etcd
      command: "kubeadm init phase etcd local --config {{ kube_reconfig_path }}/kubeadm-cluster-config-replace.yaml"

    - name: Delete kube-apiserver pod
      command: "kubectl delete pod -n kube-system kube-apiserver-{{ ansible_hostname }}"
      ignore_errors: yes

    - name: Wait for kube-apiserver pod to be running
      command: "kubectl get pod -n kube-system kube-apiserver-{{ ansible_hostname }} -o jsonpath='{.status.phase}'"
      register: check_apiserver
      retries: 10
      delay: 30
      until: check_apiserver.stdout == 'Running'

    - name: Delete kube-controller-manager pod
      command: "kubectl delete pod -n kube-system kube-controller-manager-{{ ansible_hostname }}"
      ignore_errors: yes

    - name: Wait for kube-controller-manager pod to be running
      command: "kubectl get pod -n kube-system kube-controller-manager-{{ ansible_hostname }} -o jsonpath='{.status.phase}'"
      register: check_controller_manager
      retries: 10
      delay: 30
      until: check_controller_manager.stdout == 'Running'

    - name: Delete kube-scheduler pod
      command: "kubectl delete pod -n kube-system kube-scheduler-{{ ansible_hostname }}"
      ignore_errors: yes

    - name: Wait for kube-scheduler pod to be running
      command: "kubectl get pod -n kube-system kube-scheduler-{{ ansible_hostname }} -o jsonpath='{.status.phase}'"
      register: check_scheduler
      retries: 10
      delay: 30
      until: check_scheduler.stdout == 'Running'
